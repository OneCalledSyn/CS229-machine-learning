Model Representation I
    
    -Developed to simulate neurons in the brain
    -Simple strcuture of a neuron:
        -Nucleus is the data center
        -Dendrites are the "input wires"
        -Axons are the "output wires"
    -x0 is our input node, also called the "bias unit"
    -Neural networks use the same logistic function as in classification, sometimes called sigmoid activation function
    -Theta parameters are sometimes called "weights"
    -Layer 1 (input layer) is made up of the "input nodes"
    -Layer 2 through Layer n-1 are the "hidden layers", the input layer goes into these nodes
    -Layer n is the "output layer", which finally outputs the hypothesis function
    -Hidden layer nodes are called "activation units"
    - a sub i super j = "activation" of unit i in layer j
    - theta super j = matrix of weights controlling function mapping from layer j to layer j+1
    - If network has sj units in layer j and sj+1 units in layer j+1, then Θ(j) will be of dimension s sub (j+1) × (s sub (j) + 1)
